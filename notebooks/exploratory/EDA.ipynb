{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../..\")  #Path to data directory\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "\n",
    "from matplotlib import *\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "\n",
    "from src.data_prep import *\n",
    "from src.data_constants import *\n",
    "\n",
    "from dask import dataframe as dd\n",
    "from dask_glm.datasets import make_classification\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "\n",
    "sns.set(font_scale=1.3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conda install dask\n",
    "# pip install dask-ml\n",
    "# https://spark.apache.org/docs/2.3.0/ml-classification-regression.html\n",
    "https://github.com/JIMMY-XU1/Credit_Score_Dash/blob/main/credit-Analyse%20des%20donn%C3%A9es.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_botnet_file(filename):\n",
    "    '''\n",
    "        Action: Read filename and return datafram\n",
    "    '''\n",
    "    df = dd.from_pandas(pd.DataFrame(), npartitions=2)\n",
    "    if os.path.exists(filename):\n",
    "        df = dd.read_csv(filename)\n",
    "        file_found = True\n",
    "    else:\n",
    "        print(\"File Missing: \", filename)\n",
    "        file_found = False\n",
    "    return df, file_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devices = [\"Damini_Doorbell\", \"Ennino_Doorbell\", \"Ecobee_Thermostat\", \"Ecobee_Thermostat\", \"Provision_PT_737E_Security_Camera\",\n",
    "#             \"Provision_PT_838_Security_Camera\", \"SimpleHome_XCS7_1002_WHT_Security_Camera\", \"SimpleHome_XCS7_1003_WHT_Security_Camera\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # device_list = {\"Door_Bell\": {\"Damini_Doorbell\":1, \"Ennino_Doorbell\":2},\n",
    "# #            \"Thermostat\": {\"Ecobee_Thermostat\":1},\n",
    "# #            \"Baby_Monitor\": {\"Philips_B120N10_Baby_Monitor\":1},\n",
    "# #            \"Security_Camera\": {\"Provision_PT_737E_Security_Camera\":1,\n",
    "# #                                \"Provision_PT_838_Security_Camera\":2,\n",
    "# #                                \"SimpleHome_XCS7_1002_WHT_Security_Camera\":3,\n",
    "# #                                \"SimpleHome_XCS7_1003_WHT_Security_Camera\":4}}\n",
    "\n",
    "# #Traffic types\n",
    "# BENIGN = 0\n",
    "# MIRAI = 1\n",
    "# BASHLITE = 2\n",
    "\n",
    "# # Device types\n",
    "# DOOR_BELL = 1\n",
    "# THERMOSTAT = 2\n",
    "# BABY_MONITOR = 3\n",
    "# WEB_CAMERA = 4\n",
    "# SECURITY_CAMERA = 5\n",
    "\n",
    "# # Devices\n",
    "# DAMINI_DOOR_BELL = [DOOR_BELL, \"Damini_Doorbell\"]\n",
    "# ENNINO_DOOR_BELL = [DOOR_BELL, \"Ennino_Doorbell\"]\n",
    "# ECOBEE_THERMOSTAT = [THERMOSTAT, \"Ecobee_Thermostat\"]\n",
    "# PHILIPS_BABY_MONITOR = [BABY_MONITOR, \"Philips_B120N10_Baby_Monitor\"]\n",
    "# SAMSUNG_WEB_CAMERA = [WEB_CAMERA, \"Samsung_SNH_1011_N_Webcam\"]\n",
    "# PROVISION_737E_SECURITY_CAMERA = [SECURITY_CAMERA, \"Provision_PT_737E_Security_Camera\"]\n",
    "# PROVISION_838_SECURITY_CAMERA = [SECURITY_CAMERA, \"Provision_PT_838_Security_Camera\"]\n",
    "# SIMPLEHOME_1002_SECURITY_CAMERA = [SECURITY_CAMERA, \"SimpleHome_XCS7_1002_WHT_Security_Camera\"]\n",
    "# SIMPLEHOME_1003_SECURITY_CAMERA = [SECURITY_CAMERA, \"SimpleHome_XCS7_1003_WHT_Security_Camera\"]\n",
    "\n",
    "# # Malware list\n",
    "# malwares = [MIRAI, BASHLITE]\n",
    "# # Device Type List\n",
    "# device_types = [DOOR_BELL,THERMOSTAT, BABY_MONITOR, WEB_CAMERA, SECURITY_CAMERA]\n",
    "\n",
    "# #Devices List\n",
    "# iot_devices = [ DAMINI_DOOR_BELL, ENNINO_DOOR_BELL, ECOBEE_THERMOSTAT, PHILIPS_BABY_MONITOR,SAMSUNG_WEB_CAMERA, \n",
    "#                PROVISION_737E_SECURITY_CAMERA, PROVISION_838_SECURITY_CAMERA, SIMPLEHOME_1002_SECURITY_CAMERA, SIMPLEHOME_1003_SECURITY_CAMERA ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for device in iot_devices:\n",
    "#         print(device[0], device[1])\n",
    "iot_device[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Num:0 Device Type: 0, Device NameDamini_Doorbell\n",
      "Preparing ***** Damini_Doorbell data ***** \n",
      "After Benign.. Damini_Doorbell-Damini_Doorbell: (Delayed('int-b8709261-89f6-4542-bfad-455ec1560518'), 119)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "append doesn't support list or dict input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-fd88cf6eba69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mdf_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Device_Type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mdf_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Traffic_Type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMIRAI_ACK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mdf_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m#2 scan.csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/dask/dataframe/core.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, interleave_partitions)\u001b[0m\n\u001b[1;32m   4158\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_series_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4159\u001b[0m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterleave_partitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterleave_partitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4162\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mderived_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/dask/dataframe/core.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, interleave_partitions)\u001b[0m\n\u001b[1;32m   2433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2434\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"append doesn't support list or dict input\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2435\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2437\u001b[0m         return concat(\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: append doesn't support list or dict input"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    For every device do the following:\n",
    "        1) Setup path to read files in data directory\n",
    "        2) Add to df_all dataframe from benign csv data\n",
    "            a) Add benigntraffic class as BENIGN\n",
    "            b) Add device as the intereted IoT device\n",
    "            c) Add device class as the respective device class (Door Bell, Thermostat, ..)\n",
    "            d) Add the malware type as BENIGN or MIRAI or BASHLITE\n",
    "            e) Add to all_benign_df, as a collection of benign traffic from all devices\n",
    "        2. Create df_mirai dataframe from benign csv file\n",
    "            \n",
    "''' \n",
    "\n",
    "# Initialise df_all dataframes to store all devices traffic data \n",
    "\n",
    "for iot_device in IOT_DEVICES:\n",
    "    \n",
    "    device_type = iot_device[0]   # IOT_device_type\n",
    "    device_num = iot_device[1]\n",
    "    device_name = DEVICE_LST[device_num]\n",
    "    print(f\"Device Num:{device_num} Device Type: {device_type}, Device Name{device_name}\")\n",
    "    # First path to benign, mirai, gafgyt data files\n",
    "    print(f\"Preparing ***** {device_name} data ***** \")\n",
    "    \n",
    "    # Read benign traffic for the device\n",
    "    benign_data_file = \"../../data/\"+ device_name +\"/benign_traffic.csv\"\n",
    "    \n",
    "    # Create empty dask dataframe \n",
    "    df_all = dd.from_pandas(pd.DataFrame(), npartitions=2)\n",
    "    df_temp = dd.from_pandas(pd.DataFrame(), npartitions=2)\n",
    "    \n",
    "    #Benign dataframe\n",
    "    df_temp,ff = read_botnet_file(benign_data_file)\n",
    "    df_temp['Class'] = BENIGN\n",
    "    df_temp['Device'] = device_num\n",
    "    df_temp[\"Device_Type\"] = device_type\n",
    "    df_temp[\"Traffic_Type\"] = BENIGN\n",
    "    \n",
    "#   df_temp.to_csv(\"../../data_prep/\"+device_name+\"_Benign_\"+\"benign_traffic.csv\")\n",
    "\n",
    "    # Append benign dataframe for this device to a huge benign dataframe\n",
    "#    df_all = dd.df_all.append(df_temp)  \n",
    "    df_all = df_all.append(df_temp,df_temp)\n",
    "    print(\"After Benign.. {}-{}: {}\".format(DEVICE_LST[device_type], device_name, df_all.shape))\n",
    "\n",
    "    '''\n",
    "        For the device, create mirai dataframes for ack, scan, syn, udp, udpplain attack dataframes\n",
    "    '''\n",
    "    # Mirai file path for each device\n",
    "    mirai_path = \"../../data/\"+ device_name +\"/mirai_attacks/\"\n",
    "   \n",
    "    # 1. ack.csv\n",
    "    fn = mirai_path+\"ack.csv\"\n",
    "    df_temp, ff = read_botnet_file(fn)\n",
    "    if ff: \n",
    "        df_temp['Class'] = MIRAI\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = MIRAI_ACK\n",
    "        df_all = df_all.append([df_temp])\n",
    "\n",
    "    #2 scan.csv    \n",
    "    fn = mirai_path+\"scan.csv\"\n",
    "    df_temp, ff = read_botnet_file(fn)\n",
    "    if ff: \n",
    "        df_temp['Class'] = MIRAI\n",
    "        df_temp['Device'] = device_num\n",
    "        df_temp['Device_Type'] = device_type\n",
    "        df_temp['Traffic_Type'] = MIRAI_SCAN\n",
    "        df_all = df_all.append([df_temp])       \n",
    "\n",
    "    #3 syn.csv    \n",
    "    fn = mirai_path+\"syn.csv\"        \n",
    "    df_temp, ff = read_botnet_file(fn)       \n",
    "    if ff: \n",
    "        df_temp['Class'] = MIRAI\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = MIRAI_SYN\n",
    "        df_all = df_all.append([df_temp])\n",
    "\n",
    "    #4 udp.csv    \n",
    "    fn = mirai_path+\"udp.csv\"\n",
    "    df_temp, ff = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = MIRAI\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = MIRAI_UDP\n",
    "        df_all = df_all.append([df_temp])\n",
    "\n",
    "    #5 udpplain.csv    \n",
    "    fn = mirai_path+\"udpplain.csv\"\n",
    "    df_temp,ff  = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = MIRAI\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = MIRAI_UDPPLAIN\n",
    "        df_all = df_all.append([df_temp])\n",
    "        \n",
    "    print(\"After Mirai.. {}-{}: {}\".format(DEVICE_LST[device_type], device_name, df_all.shape))\n",
    "    '''\n",
    "        For the device, create gafgyt attack dataframes for combo, junk, scan, tcp and udp attack dataframes\n",
    "    '''\n",
    "    gafgyt_path = \"../../data/\"+ device_name +\"/gafgyt_attacks/\"\n",
    "\n",
    "    # 1. combo.csv\n",
    "    fn = gafgyt_path+\"combo.csv\"\n",
    "    df_temp, ff = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = BASHLITE\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = BASHLITE_COMBO\n",
    "        df_all = df_all.append([df_temp])\n",
    "\n",
    "    #2 junk.csv    \n",
    "    fn = gafgyt_path+\"junk.csv\"\n",
    "    \n",
    "    df_temp,ff = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = BASHLITE\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = BASHLITE_JUNK\n",
    "        df_all = df_all.append([df_temp])\n",
    "\n",
    "    #3 scan.csv    \n",
    "    fn = gafgyt_path+\"scan.csv\"\n",
    "    df_temp, ff = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = BASHLITE\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = BASHLITE_SCAN\n",
    "        df_all = df_all.append([df_temp])\n",
    "\n",
    "    #4 tcp.csv    \n",
    "    fn = gafgyt_path+\"tcp.csv\"\n",
    "    df_temp, ff = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = BASHLITE\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = BASHLITE_TCP\n",
    "        df_all = df_all.append([df_temp])\n",
    "\n",
    "    #5 udp.csv    \n",
    "    fn = gafgyt_path+\"udp.csv\"\n",
    "    df_temp,ff = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = BASHLITE\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = BASHLITE_UDP\n",
    "        df_all = df_all.append([df_temp])\n",
    "        \n",
    "    print(\"After Bashlite {}-{}: {}\".format(DEVICE_LST[device_type], device_name, df_all.shape))\n",
    "    dd.df_all.to_csv(\"../../data_prep/\"+device_name+\"_traffic.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.dataframe.core.DataFrame"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame()\n",
    "df = dd.from_pandas(pd.DataFrame(), npartitions=2)\n",
    "type(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **** Delete everything below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 0 entries\n",
      "Empty DataFrame"
     ]
    }
   ],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-28fc70ee8454>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-28fc70ee8454>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://towardsdatascience.com/how-to-analyse-100s-of-gbs-of-data-on-your-laptop-with-python-f83363dda94\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "https://towardsdatascience.com/how-to-analyse-100s-of-gbs-of-data-on-your-laptop-with-python-f83363dda94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_mirai_df.shape)\n",
    "print(all_gafgyt_df.shape)\n",
    "print(all_benign_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mirai\n",
    "values = []\n",
    "for df in mirai_df:\n",
    "    values.append(len(df))\n",
    "labels=[\"mirai_ack\", \"mirai_scan\", \"mirai_syn\", \"mirai_udp\", \"mirai_udpplain\"]\n",
    "    \n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.barplot(x=labels, y=values)\n",
    "ax.set_xticklabels(labels, rotation=60)\n",
    "ax.set_title(\"Mirai Dataset\",fontsize=24 )\n",
    "\n",
    "for i, p in enumerate(ax.patches):\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2., height + 0.1, values[i],ha=\"center\")\n",
    " \n",
    "# Gaftyt\n",
    "values = []\n",
    "for df in gafgyt_df:\n",
    "    values.append(len(df))\n",
    "labels=[\"gafgyt_combo\", \"gafgyt_junk\", \"gafgyt_scan\", \"gafgyt_tcp\", \"gafgyt_udp\"]\n",
    "    \n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.barplot(x=labels, y=values)\n",
    "ax.set_xticklabels(labels, rotation=60)\n",
    "ax.set_title(\"Gafgyt Dataset\", fontsize=24)\n",
    "\n",
    "for i, p in enumerate(ax.patches):\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2., height + 0.1, values[i],ha=\"center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_pickling.py\n",
    "\n",
    "import pickle\n",
    "\n",
    "class foobar:\n",
    "    def __init__(self):\n",
    "        self.a = 35\n",
    "        self.b = \"test\"\n",
    "        self.c = lambda x: x * x\n",
    "\n",
    "    def __getstate__(self):\n",
    "        attributes = self.__dict__.copy()\n",
    "        del attributes['c']\n",
    "        return attributes\n",
    "\n",
    "my_foobar_instance = foobar()\n",
    "my_pickle_string = pickle.dumps(my_foobar_instance)\n",
    "my_new_instance = pickle.loads(my_pickle_string)\n",
    "\n",
    "print(my_new_instance.__dict__)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "26\n",
    "27\n",
    "# Save Model Using Pickle\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "# Fit the model on training set\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from Krish Naik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vaex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "n_rows =1000000\n",
    "n_cols = 500\n",
    "\n",
    "# Read  data file\n",
    "df = pd.DataFrame(np.random.randomint(0,100, size(n_rows)))\n",
    "\n",
    "df.info(memorty_usage='deep')\n",
    "\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "vaex_df = vaex.from_csv(file_path, convert=True, chunk_size=5_000_000)\n",
    "\n",
    "tuype(vaex_df)\n",
    "vaex_df = vaex.open('final_data.csv.hdf5')\n",
    "\n",
    "vaex_df.head()\n",
    "\n",
    "dff = vaex_df[vaex_fcol2>70]\n",
    "\n",
    "dff.col2.mean(progress='widget')\n",
    "\n",
    "vaex_df.group=vaex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask as dd\n",
    "import dask_ml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask as dd\n",
    "import dask_ml\n",
    "import dask.array as da\n",
    "\n",
    "#using arange to create an array with values from 0 to 10\n",
    "X = da.arange(11, chunks=5)\n",
    "X.compute()\n",
    "#to see size of each chunk\n",
    "X.chunks\n",
    "\n",
    "# Convert a numpy array to Dask array\n",
    "x = np.arange(10)\n",
    "y = da.from_array(x, chunks=5)\n",
    "y.compute() #results in a dask array\n",
    "\n",
    "# Read file\n",
    "%time temp = pd.read_csv(\"file_name.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as dd\n",
    "from dask_glm.datasets import make_classification\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import dask.dataframe as dd\n",
    "# import dask.array as da\n",
    "\n",
    "df_temp = df_all.copy()\n",
    "y = df_temp[\"Class\"]\n",
    "X = df_temp.drop(\"Class\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# from sklearn.datasets import load_digits\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import metrics\n",
    "# from dask_yarn import YarnCluster\n",
    "# from dask.distributed import Client\n",
    "# from dask_ml.linear_model import LogisticRegression\n",
    "# import dask.dataframe as dd\n",
    "# import dask.array as da\n",
    "# digits = load_digits()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "lr = LogisticRegression(solver_kwargs={\"normalize\":False})\n",
    "lr.fit(x_train, y_train)\n",
    "score = lr.score(x_test, y_test)\n",
    "print(score)\n",
    "predictions = lr.predict(x_test)\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "\n",
    "df = dd.read_csv(\"file_name.csv\")\n",
    "X = df.drop(labels=[\"Class\"],axis=1)\n",
    "y = df[\"Class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "# lr = LogisticRegression(solver_kwargs={\"normalize\": False})\n",
    "# lr.fit(X, y)\n",
    "# predictions = lr.predict(X)\n",
    "# print('predictions = {}'.format(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\").load(\"./file_name.csv\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    For every device do the following:\n",
    "        1) Setup path to read files in data directory\n",
    "        2) Add to df_all dataframe from benign csv data\n",
    "            a) Add benigntraffic class as BENIGN\n",
    "            b) Add device as the intereted IoT device\n",
    "            c) Add device class as the respective device class (Door Bell, Thermostat, ..)\n",
    "            d) Add the malware type as BENIGN or MIRAI or BASHLITE\n",
    "            e) Add to all_benign_df, as a collection of benign traffic from all devices\n",
    "        2. Create df_mirai dataframe from benign csv file\n",
    "            \n",
    "''' \n",
    "\n",
    "# Initialise df_all dataframes to store all devices traffic data \n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "for iot_device in IOT_DEVICES:\n",
    "    df_temp = pd.DataFrame()\n",
    "    device_type = iot_device[0]   # IOT_device_type\n",
    "    device_num = iot_device[1]\n",
    "    device_name = DEVICE_LST[device_num]\n",
    "    print(f\"Device Num:{device_num} Device Type: {device_type}, Device Name{device_name}\")\n",
    "    # First path to benign, mirai, gafgyt data files\n",
    "    print(f\"Preparing ***** {device_name} data ***** \")\n",
    "    \n",
    "    # Read benign traffic for the device\n",
    "    benign_data_file = \"../../data/\"+ device_name +\"/benign_traffic.csv\"\n",
    "    \n",
    "    df_temp = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]})  \n",
    "    \n",
    "    \n",
    "    df_temp = pd.DataFrame()   \n",
    "    #Benign dataframe\n",
    "    df_temp,ff = read_botnet_file(benign_data_file)\n",
    "    df_temp2 = df_temp.copy()\n",
    "    df_temp['Class'] = BENIGN\n",
    "    df_temp['Device'] = device_num\n",
    "    df_temp[\"Device_Type\"] = device_type\n",
    "    df_temp[\"Traffic_Type\"] = BENIGN\n",
    "\n",
    "    # Append benign dataframe for this device to a huge benign dataframe\n",
    "    df_all = df_all.append(df_temp)    \n",
    "    print(\"After Benign.. {}-{}: {}\".format(DEVICE_LST[device_type], device_name, df_all.shape))\n",
    "\n",
    "    '''\n",
    "        For the device, create mirai dataframes for ack, scan, syn, udp, udpplain attack dataframes\n",
    "    '''\n",
    "    # Mirai file path for each device\n",
    "    mirai_path = \"../../data/\"+ device_name +\"/mirai_attacks/\"\n",
    "   \n",
    "    # 1. ack.csv\n",
    "    fn = mirai_path+\"ack.csv\"\n",
    "    df_temp, ff = read_botnet_file(fn)\n",
    "    if ff: \n",
    "        df_temp['Class'] = MIRAI\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = MIRAI_ACK\n",
    "        df_all = df_all.append([df_temp])\n",
    "\n",
    "    #2 scan.csv    \n",
    "    fn = mirai_path+\"scan.csv\"\n",
    "    df_temp, ff = read_botnet_file(fn)\n",
    "    if ff: \n",
    "        df_temp['Class'] = MIRAI\n",
    "        df_temp['Device'] = device_num\n",
    "        df_temp['Device_Type'] = device_type\n",
    "        df_temp['Traffic_Type'] = MIRAI_SCAN\n",
    "        df_all = df_all.append([df_temp])       \n",
    "\n",
    "    #3 syn.csv    \n",
    "    fn = mirai_path+\"syn.csv\"        \n",
    "    df_temp, ff = read_botnet_file(fn)       \n",
    "    if ff: \n",
    "        df_temp['Class'] = MIRAI\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = MIRAI_SYN\n",
    "        df_all = df_all.append([df_temp])\n",
    "\n",
    "    #4 udp.csv    \n",
    "    fn = mirai_path+\"udp.csv\"\n",
    "    df_temp, ff = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = MIRAI\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = MIRAI_UDP\n",
    "        df_all = df_all.append([df_temp])\n",
    "\n",
    "    #5 udpplain.csv    \n",
    "    fn = mirai_path+\"udpplain.csv\"\n",
    "    df_temp,ff  = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = MIRAI\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = MIRAI_UDPPLAIN\n",
    "        df_all = df_all.append([df_temp])\n",
    "        \n",
    "    print(\"After Mirai.. {}-{}: {}\".format(DEVICE_LST[device_type], device_name, df_all.shape))\n",
    "    '''\n",
    "        For the device, create gafgyt attack dataframes for combo, junk, scan, tcp and udp attack dataframes\n",
    "    '''\n",
    "    gafgyt_path = \"../../data/\"+ device_name +\"/gafgyt_attacks/\"\n",
    "\n",
    "    # 1. combo.csv\n",
    "    fn = gafgyt_path+\"combo.csv\"\n",
    "    df_temp, ff = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = BASHLITE\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = BASHLITE_COMBO\n",
    "        df_all = df_all.append([df_temp])\n",
    "\n",
    "\n",
    "    #2 junk.csv    \n",
    "    fn = gafgyt_path+\"junk.csv\"\n",
    "    \n",
    "    df_temp,ff = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = BASHLITE\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = BASHLITE_JUNK\n",
    "        df_all = df_all.append([df_temp])\n",
    "\n",
    "    #3 scan.csv    \n",
    "    fn = gafgyt_path+\"scan.csv\"\n",
    "    df_temp, ff = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = BASHLITE\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = BASHLITE_SCAN\n",
    "        df_all = df_all.append([df_temp])\n",
    "\n",
    "    #4 tcp.csv    \n",
    "    fn = gafgyt_path+\"tcp.csv\"\n",
    "    df_temp, ff = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = BASHLITE\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = BASHLITE_TCP\n",
    "        df_all = df_all.append([df_temp])\n",
    "\n",
    "    #5 udp.csv    \n",
    "    fn = gafgyt_path+\"udp.csv\"\n",
    "    df_temp,ff = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = BASHLITE\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = BASHLITE_UDP\n",
    "        df_all = df_all.append([df_temp])\n",
    "        \n",
    "    print(\"After Bashlite {}-{}: {}\".format(DEVICE_LST[device_type], device_name, df_all.shape))\n",
    "df_all.to_csv('file_name.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
