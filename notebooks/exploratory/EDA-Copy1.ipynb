{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lalithap/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/lalithap/anaconda3/envs/learn-env/lib/python3.8/site-packages/lightgbm/__init__.py:42: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  warnings.warn(\"Starting from version 2.2.1, the library file in distribution wheels for macOS \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../..\")  #Path to data directory\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "\n",
    "from matplotlib import *\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "\n",
    "from src.data_prep import *\n",
    "from src.data_constants import *\n",
    "\n",
    "from dask import dataframe as dd\n",
    "from dask_glm.datasets import make_classification\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "from dask_yarn import YarnCluster\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore', category=tables.NaturalNameWarning)\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda install dask\n",
    "pip install dask-ml\n",
    "pip install tables\n",
    "pip install dask_yarn\n",
    "\n",
    "Dask Library:\n",
    "https://docs.dask.org/en/latest/dataframe-api.html\n",
    "\n",
    "https://github.com/dask/dask-tutorial/blob/master/07_dataframe_storage.ipynb\n",
    "Logistic Regression \n",
    "\n",
    "https://stackoverflow.com/questions/57295274/different-results-from-scikit-learn-and-dask-ml-logisticregression\n",
    "\n",
    "https://towardsdatascience.com/logistic-regression-model-tuning-with-scikit-learn-part-1-425142e01af5\n",
    "https://ml.dask.org/cross_validation.html\n",
    "\n",
    "pipeline\n",
    "https://medium.com/analytics-vidhya/getting-started-with-scikit-learn-pipelines-for-machine-learning-fa88efdca3b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_botnet_file(filename):\n",
    "    '''\n",
    "        Action: Read filename and return Dash dataframe\n",
    "        \n",
    "    '''\n",
    "    df = dd.from_pandas(pd.DataFrame(), npartitions=2)\n",
    "    if os.path.exists(filename):\n",
    "        df = dd.read_csv(filename)\n",
    "        file_found = True\n",
    "    else:\n",
    "        print(\"File Missing: \", filename)\n",
    "        file_found = False\n",
    "    return df, file_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading.. (0) Damini_Doorbell->Damini_Doorbell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lalithap/anaconda3/envs/learn-env/lib/python3.8/site-packages/dask/dataframe/core.py:6194: UserWarning: Insufficient elements for `head`. 5 elements requested, only 0 elements available. Try passing larger `npartitions` to `head`.\n",
      "  warnings.warn(msg.format(n, len(r)))\n",
      "/Users/lalithap/anaconda3/envs/learn-env/lib/python3.8/site-packages/tables/path.py:155: NaturalNameWarning: object name is not a valid Python identifier: '..'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 s, sys: 4.13 s, total: 18.6 s\n",
      "Wall time: 18.9 s\n",
      "Done .. Door Bell->Damini_Doorbell: Shape: (1018298,119)\n"
     ]
    }
   ],
   "source": [
    "# Takes 30 minutes to read 98 data files of 8GB classify and write to a separate file\n",
    "'''\n",
    "    For every device do the following:\n",
    "        1) Setup path to read files in data directory\n",
    "        2) Add to df_all dataframe from benign csv data\n",
    "            a) Add benigntraffic class as BENIGN\n",
    "            b) Add device as the intereted IoT device\n",
    "            c) Add device class as the respective device class (Door Bell, Thermostat, ..)\n",
    "            d) Add the malware type as BENIGN or MIRAI or BASHLITE\n",
    "            e) Add to df_all (use Dash dataframe to speedup)\n",
    "        3. Write to device csv in data_prep directory         \n",
    "''' \n",
    "\n",
    "# Initialise df_all dataframes to store all devices traffic data \n",
    "for iot_device in IOT_DEVICES:\n",
    "    \n",
    "    device_type = iot_device[0]   # IOT_device_type\n",
    "    device_num = iot_device[1]\n",
    "    device_name = DEVICE_LST[device_num]\n",
    "    \n",
    "    # First path to benign, mirai, gafgyt data files\n",
    "\n",
    "    \n",
    "    # Read benign traffic for the device\n",
    "    benign_data_file = \"../../data/\"+ device_name +\"/benign_traffic.csv\"\n",
    "    \n",
    "    # Create empty dask dataframe \n",
    "    df_all = dd.from_pandas(pd.DataFrame(), npartitions=2)\n",
    "    df_temp = dd.from_pandas(pd.DataFrame(), npartitions=2)\n",
    "    \n",
    "    #Benign dataframe\n",
    "    print(\"Reading.. ({}) {}->{}\".format(device_num, DEVICE_LST[device_type], device_name))\n",
    "    df_temp,ff = read_botnet_file(benign_data_file)\n",
    "    df_temp['Class'] = BENIGN\n",
    "    df_temp['Device'] = device_num\n",
    "    df_temp[\"Device_Type\"] = device_type\n",
    "    df_temp[\"Traffic_Type\"] = BENIGN\n",
    "    \n",
    "#   df_temp.to_csv(\"../../data_prep/\"+device_name+\"_Benign_\"+\"benign_traffic.csv\")\n",
    "\n",
    "    # Append benign dataframe for this device to a huge benign dataframe\n",
    "#    df_all = dd.df_all.append(df_temp)  \n",
    "    df_all = df_all.append(df_temp)\n",
    "#    print(\"After Benign.. {}-{}: {}\".format(DEVICE_LST[device_type], device_name, df_all.shape))\n",
    "\n",
    "    '''\n",
    "        For the device, create mirai dataframes for ack, scan, syn, udp, udpplain attack dataframes\n",
    "    '''\n",
    "    # Mirai file path for each device\n",
    "    mirai_path = \"../../data/\"+ device_name +\"/mirai_attacks/\"\n",
    "   \n",
    "    # 1. ack.csv\n",
    "    fn = mirai_path+\"ack.csv\"\n",
    "    df_temp, ff = read_botnet_file(fn)\n",
    "    if ff: \n",
    "        df_temp['Class'] = MIRAI\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = MIRAI_ACK\n",
    "        df_all = df_all.append(df_temp)\n",
    "        \n",
    "        df_all.head()\n",
    "\n",
    "    #2 scan.csv    \n",
    "    fn = mirai_path+\"scan.csv\"\n",
    "    df_temp, ff = read_botnet_file(fn)\n",
    "    if ff: \n",
    "        df_temp['Class'] = MIRAI\n",
    "        df_temp['Device'] = device_num\n",
    "        df_temp['Device_Type'] = device_type\n",
    "        df_temp['Traffic_Type'] = MIRAI_SCAN\n",
    "        df_all = df_all.append(df_temp)       \n",
    "\n",
    "    #3 syn.csv    \n",
    "    fn = mirai_path+\"syn.csv\"        \n",
    "    df_temp, ff = read_botnet_file(fn)       \n",
    "    if ff: \n",
    "        df_temp['Class'] = MIRAI\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = MIRAI_SYN\n",
    "        df_all = df_all.append(df_temp)\n",
    "\n",
    "    #4 udp.csv    \n",
    "    fn = mirai_path+\"udp.csv\"\n",
    "    df_temp, ff = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = MIRAI\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = MIRAI_UDP\n",
    "        df_all = df_all.append(df_temp)\n",
    "\n",
    "    #5 udpplain.csv    \n",
    "    fn = mirai_path+\"udpplain.csv\"\n",
    "    df_temp,ff  = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = MIRAI\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = MIRAI_UDPPLAIN\n",
    "        df_all = df_all.append(df_temp)\n",
    "        \n",
    "#    print(\"After Mirai.. {}-{}: {}\".format(DEVICE_LST[device_type], device_name, df_all.shape))\n",
    "    '''\n",
    "        For the device, create gafgyt attack dataframes for combo, junk, scan, tcp and udp attack dataframes\n",
    "    '''\n",
    "    gafgyt_path = \"../../data/\"+ device_name +\"/gafgyt_attacks/\"\n",
    "\n",
    "    # 1. combo.csv\n",
    "    fn = gafgyt_path+\"combo.csv\"\n",
    "    df_temp, ff = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = BASHLITE\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = BASHLITE_COMBO\n",
    "        df_all = df_all.append(df_temp)\n",
    "\n",
    "    #2 junk.csv    \n",
    "    fn = gafgyt_path+\"junk.csv\"\n",
    "    \n",
    "    df_temp,ff = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = BASHLITE\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = BASHLITE_JUNK\n",
    "        df_all = df_all.append(df_temp)\n",
    "\n",
    "    #3 scan.csv    \n",
    "    fn = gafgyt_path+\"scan.csv\"\n",
    "    df_temp, ff = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = BASHLITE\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = BASHLITE_SCAN\n",
    "        df_all = df_all.append(df_temp)\n",
    "\n",
    "    #4 tcp.csv    \n",
    "    fn = gafgyt_path+\"tcp.csv\"\n",
    "    df_temp, ff = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = BASHLITE\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = BASHLITE_TCP\n",
    "        df_all = df_all.append(df_temp)\n",
    "\n",
    "    #5 udp.csv    \n",
    "    fn = gafgyt_path+\"udp.csv\"\n",
    "    df_temp,ff = read_botnet_file(fn) \n",
    "    if ff: \n",
    "        df_temp['Class'] = BASHLITE\n",
    "        df_temp[\"Device\"] = device_num\n",
    "        df_temp[\"Device_Type\"] = device_type\n",
    "        df_temp[\"Traffic_Type\"] = BASHLITE_UDP\n",
    "        df_all = df_all.append(df_temp)\n",
    "        \n",
    "\n",
    "\n",
    "    outfile = os.path.join('../../data_prep/', f\"{device_name}_traffic.h5\")\n",
    "    %time df_all.to_hdf(outfile, '../../data_prep/',  mode='w')\n",
    "    print(\"Done .. {}->{}: Shape: ({},{})\".format(IOT_TYPE_LST[device_type], device_name, len(df_all), df_all.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined datafile Shape: (7574739,119)\n"
     ]
    }
   ],
   "source": [
    "# Read all files into a monster bigdata dataframe\n",
    "target = os.path.join('../../data_prep/', '*.h5')\n",
    "df_new = dd.read_hdf(target, '../../data_prep/')\n",
    "print(\"Combined datafile Shape: ({},{})\".format(len(df_new), df_new.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask as dd\n",
    "import dask_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'StandarScaler' from 'dask_ml.linear_model' (/Users/lalithap/anaconda3/envs/learn-env/lib/python3.8/site-packages/dask_ml/linear_model/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-d4fe09273887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask_yarn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYarnCluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdask_ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandarScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask_ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'StandarScaler' from 'dask_ml.linear_model' (/Users/lalithap/anaconda3/envs/learn-env/lib/python3.8/site-packages/dask_ml/linear_model/__init__.py)"
     ]
    }
   ],
   "source": [
    "import dask as dd\n",
    "import dask_ml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from dask_yarn import YarnCluster\n",
    "from dask.distributed import Client\n",
    "from dask_ml.linear_model import LogisticRegression \n",
    "from dask_ml.model_selection import train_test_split\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "\n",
    "# Read all hdf files into a monster bigdata dash dataframe\n",
    "target = os.path.join('../../data_prep/', '*.h5')\n",
    "data = dd.read_hdf(target, '../../data_prep/')\n",
    "print(\"Dataset Shape: ({},{})\".format(len(df_new), df_new.shape[1]))\n",
    "\n",
    "y = data[\"Class\"]\n",
    "X = data.drop(\"Class\", axis=1)\n",
    "\n",
    "print(\"starting tts\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X , y, test_size=0.25, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LogisticRegression\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting LogisticRegression\")\n",
    "lr = LogisticRegression(solver_kwargs={\"normalize\":False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not find signature for add_intercept: <DataFrame>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/multipledispatch/dispatcher.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (<class 'dask.dataframe.core.DataFrame'>,)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-4311d9257fbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/dask_ml/linear_model/glm.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobjectj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \"\"\"\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0msolver_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_solver_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/dask_ml/linear_model/glm.py\u001b[0m in \u001b[0;36m_check_array\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_intercept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_unknown_chunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/multipledispatch/dispatcher.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                 raise NotImplementedError(\n\u001b[0m\u001b[1;32m    274\u001b[0m                     \u001b[0;34m'Could not find signature for %s: <%s>'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                     (self.name, str_signature(types)))\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not find signature for add_intercept: <DataFrame>"
     ]
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.dataframe.core.DataFrame"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'Class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-ec7036d5b4be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'Class'"
     ]
    }
   ],
   "source": [
    "print(\"Starting LogisticRegression\")\n",
    "lr = LogisticRegression(solver_kwargs={\"normalize\":False})\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "score = lr.score(X_test, y_test)\n",
    "print(\"Score: \",score)\n",
    "\n",
    "predictions = lr.predict(x_test)\n",
    "\n",
    "print(\"Confustion Matrix: \\n\")\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Can not add intercept to array with unknown chunk shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-227b5b528c42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/dask_ml/linear_model/glm.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobjectj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \"\"\"\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0msolver_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_solver_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/dask_ml/linear_model/glm.py\u001b[0m in \u001b[0;36m_check_array\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_intercept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_unknown_chunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/multipledispatch/dispatcher.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMDNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/dask_glm/utils.py\u001b[0m in \u001b[0;36madd_intercept\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_intercept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         raise NotImplementedError(\"Can not add intercept to array with \"\n\u001b[0m\u001b[1;32m    148\u001b[0m                                   \"unknown chunk shape\")\n\u001b[1;32m    149\u001b[0m     \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Can not add intercept to array with unknown chunk shape"
     ]
    }
   ],
   "source": [
    "from dask import dataframe as dd\n",
    "from dask_glm.datasets import make_classification\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "\n",
    "X, y = make_classification(n_samples=10000, n_features=2)\n",
    "\n",
    "X = dd.from_dask_array(X, columns=[\"a\",\"b\"])\n",
    "y = dd.from_array(y)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X.values, y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as dd\n",
    "from dask_glm.datasets import make_classification\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import dask.dataframe as dd\n",
    "# import dask.array as da\n",
    "\n",
    "df_temp = df_all.copy()\n",
    "y = df_temp[\"Class\"]\n",
    "X = df_temp.drop(\"Class\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# from sklearn.datasets import load_digits\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import metrics\n",
    "# from dask_yarn import YarnCluster\n",
    "# from dask.distributed import Client\n",
    "# from dask_ml.linear_model import LogisticRegression\n",
    "# import dask.dataframe as dd\n",
    "# import dask.array as da\n",
    "# digits = load_digits()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "lr = LogisticRegression(solver_kwargs={\"normalize\":False})\n",
    "lr.fit(x_train, y_train)\n",
    "score = lr.score(x_test, y_test)\n",
    "print(score)\n",
    "predictions = lr.predict(x_test)\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **** Delete everything below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/how-to-analyse-100s-of-gbs-of-data-on-your-laptop-with-python-f83363dda94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mirai\n",
    "values = []\n",
    "for df in mirai_df:\n",
    "    values.append(len(df))\n",
    "labels=[\"mirai_ack\", \"mirai_scan\", \"mirai_syn\", \"mirai_udp\", \"mirai_udpplain\"]\n",
    "    \n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.barplot(x=labels, y=values)\n",
    "ax.set_xticklabels(labels, rotation=60)\n",
    "ax.set_title(\"Mirai Dataset\",fontsize=24 )\n",
    "\n",
    "for i, p in enumerate(ax.patches):\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2., height + 0.1, values[i],ha=\"center\")\n",
    " \n",
    "# Gaftyt\n",
    "values = []\n",
    "for df in gafgyt_df:\n",
    "    values.append(len(df))\n",
    "labels=[\"gafgyt_combo\", \"gafgyt_junk\", \"gafgyt_scan\", \"gafgyt_tcp\", \"gafgyt_udp\"]\n",
    "    \n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.barplot(x=labels, y=values)\n",
    "ax.set_xticklabels(labels, rotation=60)\n",
    "ax.set_title(\"Gafgyt Dataset\", fontsize=24)\n",
    "\n",
    "for i, p in enumerate(ax.patches):\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2., height + 0.1, values[i],ha=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "\n",
    "df = dd.read_csv(\"file_name.csv\")\n",
    "X = df.drop(labels=[\"Class\"],axis=1)\n",
    "y = df[\"Class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "# lr = LogisticRegression(solver_kwargs={\"normalize\": False})\n",
    "# lr.fit(X, y)\n",
    "# predictions = lr.predict(X)\n",
    "# print('predictions = {}'.format(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_pickling.py\n",
    "\n",
    "import pickle\n",
    "\n",
    "class foobar:\n",
    "    def __init__(self):\n",
    "        self.a = 35\n",
    "        self.b = \"test\"\n",
    "        self.c = lambda x: x * x\n",
    "\n",
    "    def __getstate__(self):\n",
    "        attributes = self.__dict__.copy()\n",
    "        del attributes['c']\n",
    "        return attributes\n",
    "\n",
    "my_foobar_instance = foobar()\n",
    "my_pickle_string = pickle.dumps(my_foobar_instance)\n",
    "my_new_instance = pickle.loads(my_pickle_string)\n",
    "\n",
    "print(my_new_instance.__dict__)\n",
    "\n",
    "\n",
    "# Save Model Using Pickle\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "# Fit the model on training set\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is vaex better than Dash? don't know...\n",
    "\n",
    "import vaex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "n_rows =1000000\n",
    "n_cols = 500\n",
    "\n",
    "# Read  data file\n",
    "df = pd.DataFrame(np.random.randomint(0,100, size(n_rows)))\n",
    "\n",
    "df.info(memorty_usage='deep')\n",
    "\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "vaex_df = vaex.from_csv(file_path, convert=True, chunk_size=5_000_000)\n",
    "\n",
    "tuype(vaex_df)\n",
    "vaex_df = vaex.open('final_data.csv.hdf5')\n",
    "\n",
    "vaex_df.head()\n",
    "\n",
    "dff = vaex_df[vaex_fcol2>70]\n",
    "\n",
    "dff.col2.mean(progress='widget')\n",
    "\n",
    "vaex_df.group=vaex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask as dd\n",
    "import dask_ml\n",
    "import dask.array as da\n",
    "\n",
    "#using arange to create an array with values from 0 to 10\n",
    "X = da.arange(11, chunks=5)\n",
    "X.compute()\n",
    "#to see size of each chunk\n",
    "X.chunks\n",
    "\n",
    "# Convert a numpy array to Dask array\n",
    "x = np.arange(10)\n",
    "y = da.from_array(x, chunks=5)\n",
    "y.compute() #results in a dask array\n",
    "\n",
    "# Read file\n",
    "%time temp = pd.read_csv(\"file_name.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as dd\n",
    "from dask_glm.datasets import make_classification\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import dask.dataframe as dd\n",
    "# import dask.array as da\n",
    "\n",
    "df_temp = df_all.copy()\n",
    "y = df_temp[\"Class\"]\n",
    "X = df_temp.drop(\"Class\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# from sklearn.datasets import load_digits\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import metrics\n",
    "# from dask_yarn import YarnCluster\n",
    "# from dask.distributed import Client\n",
    "# from dask_ml.linear_model import LogisticRegression\n",
    "# import dask.dataframe as dd\n",
    "# import dask.array as da\n",
    "# digits = load_digits()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "lr = LogisticRegression(solver_kwargs={\"normalize\":False})\n",
    "lr.fit(x_train, y_train)\n",
    "score = lr.score(x_test, y_test)\n",
    "print(score)\n",
    "predictions = lr.predict(x_test)\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\").load(\"./file_name.csv\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
